{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dcd90e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b3fc886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def get_all_records_for_date(target_date, max_records=5000):\n",
    "    \"\"\"Get all records for a specific date with all available fields\"\"\"\n",
    "    url = \"https://boamp-datadila.opendatasoft.com/api/explore/v2.1/catalog/datasets/boamp/records\"\n",
    "    all_records = []\n",
    "    offset = 0\n",
    "    limit = 100\n",
    "\n",
    "    while len(all_records) < max_records:\n",
    "        params = {\n",
    "            'order_by': 'dateparution DESC',\n",
    "            'limit': limit,\n",
    "            'offset': offset\n",
    "        }\n",
    "\n",
    "        print(f\"Requesting offset {offset}...\")\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        records = data.get('results', [])\n",
    "\n",
    "        if not records:\n",
    "            break  # No more records\n",
    "\n",
    "        # Filter records for our target date\n",
    "        target_records = [record for record in records if record.get('dateparution') == target_date]\n",
    "\n",
    "        # If we found target records, add them\n",
    "        if target_records:\n",
    "            all_records.extend(target_records)\n",
    "            print(f\"Retrieved {len(target_records)} records for {target_date}... Total so far: {len(all_records)}\")\n",
    "\n",
    "        # Check if we've moved past our target date (since we're sorting DESC)\n",
    "        if records and records[-1].get('dateparution', '') < target_date:\n",
    "            print(f\"Reached dates earlier than {target_date}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        offset += limit\n",
    "\n",
    "        if offset > 10000:\n",
    "            print(\"Safety limit reached. Stopping.\")\n",
    "            break\n",
    "\n",
    "    return all_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3bf701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for BOAMP records with publication date: 2025-10-31\n",
      "============================================================\n",
      "Requesting offset 0...\n",
      "Retrieved 100 records for 2025-10-31... Total so far: 100\n",
      "Requesting offset 100...\n",
      "Retrieved 100 records for 2025-10-31... Total so far: 200\n",
      "Requesting offset 200...\n",
      "Retrieved 100 records for 2025-10-31... Total so far: 300\n",
      "Requesting offset 300...\n",
      "Retrieved 100 records for 2025-10-31... Total so far: 400\n",
      "Requesting offset 400...\n",
      "Retrieved 59 records for 2025-10-31... Total so far: 459\n",
      "Reached dates earlier than 2025-10-31. Stopping.\n",
      "\n",
      "============================================================\n",
      "EXTRACTION COMPLETE\n",
      "Found 459 records for date 2025-10-31\n",
      "Searching for BOAMP records with publication date: 2025-10-30\n",
      "============================================================\n",
      "Requesting offset 0...\n",
      "Requesting offset 100...\n",
      "Requesting offset 200...\n",
      "Requesting offset 300...\n",
      "Requesting offset 400...\n",
      "Retrieved 41 records for 2025-10-30... Total so far: 41\n",
      "Requesting offset 500...\n",
      "Retrieved 100 records for 2025-10-30... Total so far: 141\n",
      "Requesting offset 600...\n",
      "Retrieved 100 records for 2025-10-30... Total so far: 241\n",
      "Requesting offset 700...\n",
      "Retrieved 100 records for 2025-10-30... Total so far: 341\n",
      "Requesting offset 800...\n",
      "Retrieved 100 records for 2025-10-30... Total so far: 441\n",
      "Requesting offset 900...\n",
      "Retrieved 92 records for 2025-10-30... Total so far: 533\n",
      "Reached dates earlier than 2025-10-30. Stopping.\n",
      "\n",
      "============================================================\n",
      "EXTRACTION COMPLETE\n",
      "Found 533 records for date 2025-10-30\n",
      "Searching for BOAMP records with publication date: 2025-10-29\n",
      "============================================================\n",
      "Requesting offset 0...\n",
      "Requesting offset 100...\n",
      "Requesting offset 200...\n",
      "Requesting offset 300...\n",
      "Requesting offset 400...\n",
      "Requesting offset 500...\n",
      "Requesting offset 600...\n",
      "Requesting offset 700...\n",
      "Requesting offset 800...\n",
      "Requesting offset 900...\n",
      "Retrieved 8 records for 2025-10-29... Total so far: 8\n",
      "Requesting offset 1000...\n",
      "Retrieved 100 records for 2025-10-29... Total so far: 108\n",
      "Requesting offset 1100...\n",
      "Retrieved 100 records for 2025-10-29... Total so far: 208\n",
      "Requesting offset 1200...\n",
      "Retrieved 100 records for 2025-10-29... Total so far: 308\n",
      "Requesting offset 1300...\n",
      "Retrieved 100 records for 2025-10-29... Total so far: 408\n",
      "Requesting offset 1400...\n",
      "Retrieved 55 records for 2025-10-29... Total so far: 463\n",
      "Reached dates earlier than 2025-10-29. Stopping.\n",
      "\n",
      "============================================================\n",
      "EXTRACTION COMPLETE\n",
      "Found 463 records for date 2025-10-29\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    target_date = '2025-10-31'\n",
    "\n",
    "    print(f\"Searching for BOAMP records with publication date: {target_date}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Get all records\n",
    "    all_records = get_all_records_for_date(target_date)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXTRACTION COMPLETE\")\n",
    "    print(f\"Found {len(all_records)} records for date {target_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9b9de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_excel_simple(records, target_date):\n",
    "    \"\"\"Simple and robust Excel creation\"\"\"\n",
    "\n",
    "    # Clean the records\n",
    "    cleaned_records = []\n",
    "    for record in records:\n",
    "        cleaned_record = {}\n",
    "        for key, value in record.items():\n",
    "            # Handle different data types\n",
    "            if isinstance(value, (list, dict)):\n",
    "                cleaned_record[key] = json.dumps(value, ensure_ascii=False)\n",
    "            elif value is None:\n",
    "                cleaned_record[key] = ''\n",
    "            else:\n",
    "                cleaned_record[key] = value\n",
    "        cleaned_records.append(cleaned_record)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(cleaned_records)\n",
    "\n",
    "    # Create filename\n",
    "    filename = f\"BOAMP_{target_date}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "\n",
    "    # Export to Excel - SIMPLE VERSION, just the data\n",
    "    df.to_excel(filename, index=False, engine='openpyxl')\n",
    "\n",
    "    return filename, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3df9e1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Excel file created: BOAMP_2025-10-29_20251101_003445.xlsx\n",
      "üìä Total records exported: 463\n",
      "üìã Total columns: 41\n",
      "\n",
      "üìã Columns in Excel file:\n",
      "   1. idweb\n",
      "   2. id\n",
      "   3. contractfolderid\n",
      "   4. objet\n",
      "   5. filename\n",
      "   6. famille\n",
      "   7. code_departement\n",
      "   8. code_departement_prestation\n",
      "   9. famille_libelle\n",
      "  10. dateparution\n",
      "  11. datefindiffusion\n",
      "  12. datelimitereponse\n",
      "  13. nomacheteur\n",
      "  14. titulaire\n",
      "  15. perimetre\n",
      "  16. type_procedure\n",
      "  17. soustype_procedure\n",
      "  18. procedure_libelle\n",
      "  19. procedure_categorise\n",
      "  20. nature\n",
      "  21. sousnature\n",
      "  22. nature_libelle\n",
      "  23. sousnature_libelle\n",
      "  24. nature_categorise\n",
      "  25. nature_categorise_libelle\n",
      "  26. criteres\n",
      "  27. marche_public_simplifie\n",
      "  28. marche_public_simplifie_label\n",
      "  29. etat\n",
      "  30. descripteur_code\n",
      "  31. dc\n",
      "  32. descripteur_libelle\n",
      "  33. type_marche\n",
      "  34. type_marche_facette\n",
      "  35. type_avis\n",
      "  36. annonce_lie\n",
      "  37. annonces_anterieures\n",
      "  38. source_schema\n",
      "  39. gestion\n",
      "  40. donnees\n",
      "  41. url_avis\n",
      "\n",
      "üìÑ SAMPLE DATA (first 3 records):\n",
      "============================================================\n",
      "Record 1:\n",
      "  Title: Ma√Ætrise d'oeuvre du projet de stade d'eaux vives √† Ancerville coupl√© √† une micr...\n",
      "  Buyer: CC des Portes De Meuse\n",
      "  Procedure: Proc√©dure NC\n",
      "  Date: 2025-10-29\n",
      "------------------------------------------------------------\n",
      "Record 2:\n",
      "  Title: Mission de conseil et d'assistance √† ma√Ætrise d'ouvrage pour la passation des ma...\n",
      "  Buyer: Etablissement Public Foncier √éle-de-France\n",
      "  Procedure: Proc√©dure Ouverte\n",
      "  Date: 2025-10-29\n",
      "------------------------------------------------------------\n",
      "Record 3:\n",
      "  Title: March√© entretien et gros travaux des couvertures Toiture traditionnelle...\n",
      "  Buyer: Ville de Cognac\n",
      "  Procedure: Proc√©dure Ouverte\n",
      "  Date: 2025-10-29\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if all_records:\n",
    "    # Create Excel file\n",
    "    excel_filename, df = create_excel_simple(all_records, target_date)\n",
    "\n",
    "    print(f\"\\n‚úÖ Excel file created: {excel_filename}\")\n",
    "    print(f\"üìä Total records exported: {len(all_records)}\")\n",
    "    print(f\"üìã Total columns: {len(df.columns)}\")\n",
    "\n",
    "    # Show column names\n",
    "    print(f\"\\nüìã Columns in Excel file:\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "    # Display sample data\n",
    "    print(f\"\\nüìÑ SAMPLE DATA (first 3 records):\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for i, record in enumerate(all_records[:3], 1):\n",
    "        print(f\"Record {i}:\")\n",
    "        print(f\"  Title: {record.get('objet', 'N/A')[:80]}...\")\n",
    "        print(f\"  Buyer: {record.get('nomacheteur', 'N/A')}\")\n",
    "        print(f\"  Procedure: {record.get('procedure_libelle', 'N/A')}\")\n",
    "        print(f\"  Date: {record.get('dateparution', 'N/A')}\")\n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"‚ùå No records found for the specified date.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd3a9839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 15 matching rows. Saved to:\n",
      "D:\\sorabo\\sorabo\\BOAMP_2025-10-31_serrurerie.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. Define your file path and keyword ---\n",
    "file_path = r\"D:\\sorabo\\sorabo\\BOAMP_2025-10-31_20251031_235226.xlsx\"\n",
    "keyword = \"serrurerie\"\n",
    "output_path = r\"D:\\sorabo\\sorabo\\BOAMP_2025-10-31_serrurerie.xlsx\"\n",
    "\n",
    "# --- 2. Load the Excel file ---\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# --- 3. Convert all text to lowercase for case-insensitive search ---\n",
    "df_str = df.astype(str).apply(lambda x: x.str.lower())\n",
    "\n",
    "# --- 4. Filter rows that contain the keyword in any column ---\n",
    "mask = df_str.apply(lambda x: x.str.contains(keyword, na=False))\n",
    "filtered_df = df[mask.any(axis=1)]\n",
    "\n",
    "# --- 5. Save the filtered data to a new Excel file ---\n",
    "filtered_df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Found {len(filtered_df)} matching rows. Saved to:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48eb513c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Highlighted all cells containing 'serrurerie' in green.\n",
      "Saved to:\n",
      "D:\\sorabo\\sorabo\\BOAMP.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "\n",
    "# --- 1. Paths & Keyword ---\n",
    "file_path = r\"D:\\sorabo\\sorabo\\BOAMP_2025-10-31_serrurerie.xlsx\"\n",
    "output_path = r\"D:\\sorabo\\sorabo\\BOAMP.xlsx\"\n",
    "keyword = \"serrurerie\"\n",
    "\n",
    "# --- 2. Load Excel file into pandas ---\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# --- 3. Save temporarily (openpyxl works on real Excel files) ---\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "# --- 4. Load workbook with openpyxl ---\n",
    "wb = load_workbook(output_path)\n",
    "ws = wb.active\n",
    "\n",
    "# --- 5. Define the green fill style ---\n",
    "green_fill = PatternFill(start_color=\"90EE90\", end_color=\"90EE90\", fill_type=\"solid\")  # Light green\n",
    "\n",
    "# --- 6. Loop through each cell and check for keyword ---\n",
    "for row in ws.iter_rows(min_row=2):  # skip header row\n",
    "    for cell in row:\n",
    "        if cell.value and keyword.lower() in str(cell.value).lower():\n",
    "            cell.fill = green_fill\n",
    "\n",
    "# --- 7. Save workbook ---\n",
    "wb.save(output_path)\n",
    "\n",
    "print(f\"‚úÖ Highlighted all cells containing '{keyword}' in green.\\nSaved to:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdd056d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Highlighted 17 cells containing 'serrurerie'.\n",
      "üíæ Saved to: D:\\sorabo\\sorabo\\BOAMP_2025-10-31_serrurerie_highlighted.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "\n",
    "# --- File paths ---\n",
    "input_path = r\"D:\\sorabo\\sorabo\\BOAMP_2025-10-31_serrurerie.xlsx\"\n",
    "output_path = r\"D:\\sorabo\\sorabo\\BOAMP_2025-10-31_serrurerie_highlighted.xlsx\"\n",
    "\n",
    "# --- Keyword to highlight ---\n",
    "keyword = \"serrurerie\"\n",
    "\n",
    "# --- Load Excel file with openpyxl directly ---\n",
    "wb = load_workbook(input_path)\n",
    "ws = wb.active\n",
    "\n",
    "# --- Define the green fill ---\n",
    "green_fill = PatternFill(start_color=\"90EE90\", end_color=\"90EE90\", fill_type=\"solid\")\n",
    "\n",
    "# --- Loop through cells and apply highlight ---\n",
    "count = 0\n",
    "for row in ws.iter_rows(min_row=1):  # Include headers if needed\n",
    "    for cell in row:\n",
    "        if cell.value and keyword.lower() in str(cell.value).lower():\n",
    "            cell.fill = green_fill\n",
    "            count += 1\n",
    "\n",
    "# --- Save the new file ---\n",
    "wb.save(output_path)\n",
    "\n",
    "print(f\"‚úÖ Highlighted {count} cells containing '{keyword}'.\")\n",
    "print(f\"üíæ Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41754c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 15 rows for 'serrurerie'. Saved to:\n",
      "D:\\sorabo\\sorabo\\BOAMP_2025-10-31_serrurerie.xlsx\n",
      "‚ö†Ô∏è No matches found for 'miroiterie'.\n",
      "‚úÖ Found 7 rows for 'm√©tallerie'. Saved to:\n",
      "D:\\sorabo\\sorabo\\BOAMP_2025-10-31_m√©tallerie.xlsx\n",
      "‚úÖ Found 4 rows for 'menuiserie ext√©rieure'. Saved to:\n",
      "D:\\sorabo\\sorabo\\BOAMP_2025-10-31_menuiserie_ext√©rieure.xlsx\n",
      "‚úÖ Found 3 rows for 'cl√¥tures'. Saved to:\n",
      "D:\\sorabo\\sorabo\\BOAMP_2025-10-31_cl√¥tures.xlsx\n",
      "‚ö†Ô∏è No matches found for '44233000'.\n",
      "‚úÖ Found 1 rows for 'escaliers'. Saved to:\n",
      "D:\\sorabo\\sorabo\\BOAMP_2025-10-31_escaliers.xlsx\n",
      "‚úÖ Found 4 rows for '44316500'. Saved to:\n",
      "D:\\sorabo\\sorabo\\BOAMP_2025-10-31_44316500.xlsx\n",
      "‚ö†Ô∏è No matches found for '45421132'.\n",
      "‚úÖ Found 2 rows for '45421140'. Saved to:\n",
      "D:\\sorabo\\sorabo\\BOAMP_2025-10-31_45421140.xlsx\n",
      "\n",
      "üìä Combined results saved to:\n",
      "D:\\sorabo\\sorabo\\BOAMP_2025-10-31_ALL_KEYWORDS.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Define paths and keywords ---\n",
    "file_path = r\"D:\\sorabo\\sorabo\\BOAMP_2025-10-31_20251031_235226.xlsx\"\n",
    "output_dir = r\"D:\\sorabo\\sorabo\"\n",
    "keywords = [\n",
    "    \"serrurerie\",\n",
    "    \"miroiterie\",\n",
    "    \"m√©tallerie\",\n",
    "    \"menuiserie ext√©rieure\",\n",
    "    \"cl√¥tures\",\n",
    "    \"44233000\",\n",
    "    \"escaliers\",\n",
    "    \"44316500\",\n",
    "    \"45421132\",\n",
    "    \"45421140\"\n",
    "]\n",
    "\n",
    "# --- 2. Load the Excel file ---\n",
    "df = pd.read_excel(file_path)\n",
    "df_str = df.astype(str).apply(lambda x: x.str.lower())  # lowercase for case-insensitive match\n",
    "\n",
    "# --- 3. Prepare a list to store all matches ---\n",
    "all_matches = pd.DataFrame()\n",
    "\n",
    "# --- 4. Loop through each keyword ---\n",
    "for keyword in keywords:\n",
    "    mask = df_str.apply(lambda x: x.str.contains(keyword.lower(), na=False))\n",
    "    filtered_df = df[mask.any(axis=1)]\n",
    "    \n",
    "    if not filtered_df.empty:\n",
    "        output_path = os.path.join(output_dir, f\"BOAMP_2025-10-31_{keyword.replace(' ', '_')}.xlsx\")\n",
    "        filtered_df.to_excel(output_path, index=False)\n",
    "        print(f\"‚úÖ Found {len(filtered_df)} rows for '{keyword}'. Saved to:\\n{output_path}\")\n",
    "        \n",
    "        # Add a column showing which keyword matched\n",
    "        filtered_df = filtered_df.copy()\n",
    "        filtered_df[\"keyword\"] = keyword\n",
    "        all_matches = pd.concat([all_matches, filtered_df], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No matches found for '{keyword}'.\")\n",
    "\n",
    "# --- 5. Save all combined matches ---\n",
    "if not all_matches.empty:\n",
    "    combined_output = os.path.join(output_dir, \"BOAMP_2025-10-31_ALL_KEYWORDS.xlsx\")\n",
    "    all_matches.to_excel(combined_output, index=False)\n",
    "    print(f\"\\nüìä Combined results saved to:\\n{combined_output}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No matches found for any keyword.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be2f1ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Reading Excel file...\n",
      "‚úÖ Found 257 rows for 'serrurerie'\n",
      "‚úÖ Found 11 rows for 'miroiterie'\n",
      "‚úÖ Found 95 rows for 'm√©tallerie'\n",
      "‚úÖ Found 30 rows for 'menuiserie ext√©rieure'\n",
      "‚úÖ Found 52 rows for 'cl√¥tures'\n",
      "‚úÖ Found 23 rows for 'escaliers'\n",
      "‚ö†Ô∏è No matches found for '44233000'.\n",
      "‚úÖ Found 73 rows for '44316500'\n",
      "‚ö†Ô∏è No matches found for '45421132'.\n",
      "‚úÖ Found 16 rows for '45421140'\n",
      "‚ö†Ô∏è No matches found for '45420000-7'.\n",
      "‚ö†Ô∏è No matches found for 'Travaux de menuiserie et de charpenterie'.\n",
      "‚úÖ Found 3 rows for '45421000-4'\n",
      "‚úÖ Found 23 rows for 'Travaux de menuiserie'\n",
      "‚ö†Ô∏è No matches found for '45421100-5'.\n",
      "‚ö†Ô∏è No matches found for 'Pose de portes et de fen√™tres et d'√©l√©ments accessoires'.\n",
      "‚ö†Ô∏è No matches found for '45421110-8'.\n",
      "‚ö†Ô∏è No matches found for 'Pose d'encadrements de portes et de fen√™tres'.\n",
      "‚ö†Ô∏è No matches found for '45421111-5'.\n",
      "‚ö†Ô∏è No matches found for 'Pose d'encadrements de portes'.\n",
      "‚ö†Ô∏è No matches found for '45421112-2'.\n",
      "‚ö†Ô∏è No matches found for 'Pose d'encadrements de fen√™tres'.\n",
      "‚ö†Ô∏è No matches found for '45421120-1'.\n",
      "‚ö†Ô∏è No matches found for 'Pose de seuils'.\n",
      "‚ö†Ô∏è No matches found for '45421130-4'.\n",
      "‚ö†Ô∏è No matches found for 'Poses de portes et de fen√™tres'.\n",
      "‚ö†Ô∏è No matches found for '45421131-1'.\n",
      "‚úÖ Found 1 rows for 'Pose de portes'\n",
      "‚ö†Ô∏è No matches found for '45421132-8'.\n",
      "‚ö†Ô∏è No matches found for 'Pose de fen√™tres'.\n",
      "‚ö†Ô∏è No matches found for '45421140-7'.\n",
      "‚ö†Ô∏è No matches found for 'Pose de menuiseries m√©talliques, except√© portes et fen√™tres'.\n",
      "‚úÖ Found 1 rows for '45421141-4'\n",
      "‚úÖ Found 4 rows for 'Travaux de cloisonnement'\n",
      "‚ö†Ô∏è No matches found for '45421142-1'.\n",
      "‚ö†Ô∏è No matches found for 'Installation de volets'.\n",
      "‚ö†Ô∏è No matches found for '45421143-8'.\n",
      "‚ö†Ô∏è No matches found for 'Travaux d'installation de stores'.\n",
      "‚ö†Ô∏è No matches found for '45421144-5'.\n",
      "‚ö†Ô∏è No matches found for 'Travaux d'installation de v√©lums'.\n",
      "‚ö†Ô∏è No matches found for '45421145-2'.\n",
      "‚ö†Ô∏è No matches found for 'Travaux d'installation de volets roulants'.\n",
      "\n",
      "üìä All matching data saved successfully to:\n",
      "D:\\sorabo\\sorabo\\BOAMP_2025-10-31_ALL_KEYWORDS.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Define your file path ---\n",
    "file_path = r\"D:\\sorabo\\sorabo\\data\\31-10_merged.xlsx\"\n",
    "output_path = r\"D:\\sorabo\\sorabo\\BOAMP_2025-10-31_ALL_KEYWORDS.xlsx\"\n",
    "\n",
    "# --- 2. Define all your keywords and CPV codes ---\n",
    "keywords = [\n",
    "    # Secteurs d‚Äôactivit√©\n",
    "    \"serrurerie\",\n",
    "    \"miroiterie\",\n",
    "    \"m√©tallerie\",\n",
    "    \"menuiserie ext√©rieure\",\n",
    "    \"cl√¥tures\",\n",
    "    \"escaliers\",\n",
    "\n",
    "    # CPV simples\n",
    "    \"44233000\",\n",
    "    \"44316500\",\n",
    "    \"45421132\",\n",
    "    \"45421140\",\n",
    "\n",
    "    # CPV d√©taill√©s avec description\n",
    "    \"45420000-7\", \"Travaux de menuiserie et de charpenterie\",\n",
    "    \"45421000-4\", \"Travaux de menuiserie\",\n",
    "    \"45421100-5\", \"Pose de portes et de fen√™tres et d'√©l√©ments accessoires\",\n",
    "    \"45421110-8\", \"Pose d'encadrements de portes et de fen√™tres\",\n",
    "    \"45421111-5\", \"Pose d'encadrements de portes\",\n",
    "    \"45421112-2\", \"Pose d'encadrements de fen√™tres\",\n",
    "    \"45421120-1\", \"Pose de seuils\",\n",
    "    \"45421130-4\", \"Poses de portes et de fen√™tres\",\n",
    "    \"45421131-1\", \"Pose de portes\",\n",
    "    \"45421132-8\", \"Pose de fen√™tres\",\n",
    "    \"45421140-7\", \"Pose de menuiseries m√©talliques, except√© portes et fen√™tres\",\n",
    "    \"45421141-4\", \"Travaux de cloisonnement\",\n",
    "    \"45421142-1\", \"Installation de volets\",\n",
    "    \"45421143-8\", \"Travaux d'installation de stores\",\n",
    "    \"45421144-5\", \"Travaux d'installation de v√©lums\",\n",
    "    \"45421145-2\", \"Travaux d'installation de volets roulants\"\n",
    "]\n",
    "\n",
    "# --- 3. Load Excel ---\n",
    "print(\"üìÇ Reading Excel file...\")\n",
    "df = pd.read_excel(file_path)\n",
    "df_str = df.astype(str).apply(lambda x: x.str.lower())  # for case-insensitive search\n",
    "\n",
    "# --- 4. Create one big dataframe for all results ---\n",
    "all_matches = pd.DataFrame()\n",
    "\n",
    "# --- 5. Loop through each keyword ---\n",
    "for keyword in keywords:\n",
    "    mask = df_str.apply(lambda x: x.str.contains(keyword.lower(), na=False))\n",
    "    filtered_df = df[mask.any(axis=1)]\n",
    "    \n",
    "    if not filtered_df.empty:\n",
    "        print(f\"‚úÖ Found {len(filtered_df)} rows for '{keyword}'\")\n",
    "        filtered_df = filtered_df.copy()\n",
    "        filtered_df[\"keyword\"] = keyword\n",
    "        all_matches = pd.concat([all_matches, filtered_df], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No matches found for '{keyword}'.\")\n",
    "\n",
    "# --- 6. Save everything to one file ---\n",
    "if not all_matches.empty:\n",
    "    all_matches.to_excel(output_path, index=False)\n",
    "    print(f\"\\nüìä All matching data saved successfully to:\\n{output_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No matches found for any keyword.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf0cb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Reading Excel file...\n",
      "‚úÖ Found 2 rows for 'serrurerie'\n",
      "‚ö†Ô∏è No matches found for 'miroiterie'.\n",
      "‚ö†Ô∏è No matches found for 'm√©tallerie'.\n",
      "‚úÖ Found 5 rows for 'menuiserie'\n",
      "‚úÖ Found 4 rows for 'cl√¥tures'\n",
      "‚úÖ Found 10 rows for 'escaliers'\n",
      "\n",
      "üìä All matching data saved successfully to:\n",
      "D:\\sorabo\\sorabo\\codesc.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Define your file path ---\n",
    "file_path = r\"D:\\sorabo\\sorabo\\data\\codes.xlsx\"\n",
    "output_path = r\"D:\\sorabo\\sorabo\\codesc.xlsx\"\n",
    "\n",
    "# --- 2. Define all your keywords and CPV codes ---\n",
    "keywords = [\n",
    "    # Secteurs d‚Äôactivit√©\n",
    "    \"serrurerie\",\n",
    "    \"miroiterie\",\n",
    "    \"m√©tallerie\",\n",
    "    \"menuiserie\",\n",
    "    \"cl√¥tures\",\n",
    "    \"escaliers\",\n",
    "\n",
    "]\n",
    "\n",
    "# --- 3. Load Excel ---\n",
    "print(\"üìÇ Reading Excel file...\")\n",
    "df = pd.read_excel(file_path)\n",
    "df_str = df.astype(str).apply(lambda x: x.str.lower())  # for case-insensitive search\n",
    "\n",
    "# --- 4. Create one big dataframe for all results ---\n",
    "all_matches = pd.DataFrame()\n",
    "\n",
    "# --- 5. Loop through each keyword ---\n",
    "for keyword in keywords:\n",
    "    mask = df_str.apply(lambda x: x.str.contains(keyword.lower(), na=False))\n",
    "    filtered_df = df[mask.any(axis=1)]\n",
    "    \n",
    "    if not filtered_df.empty:\n",
    "        print(f\"‚úÖ Found {len(filtered_df)} rows for '{keyword}'\")\n",
    "        filtered_df = filtered_df.copy()\n",
    "        filtered_df[\"keyword\"] = keyword\n",
    "        all_matches = pd.concat([all_matches, filtered_df], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No matches found for '{keyword}'.\")\n",
    "\n",
    "# --- 6. Save everything to one file ---\n",
    "if not all_matches.empty:\n",
    "    all_matches.to_excel(output_path, index=False)\n",
    "    print(f\"\\nüìä All matching data saved successfully to:\\n{output_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No matches found for any keyword.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37f6c252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Searching for BOAMP records with publication date: 2025-10-31\n",
      "============================================================\n",
      "Requesting offset 0...\n",
      "Retrieved 100 records for 2025-10-31... Total so far: 100\n",
      "Requesting offset 100...\n",
      "Retrieved 100 records for 2025-10-31... Total so far: 200\n",
      "Requesting offset 200...\n",
      "Retrieved 100 records for 2025-10-31... Total so far: 300\n",
      "Requesting offset 300...\n",
      "Retrieved 100 records for 2025-10-31... Total so far: 400\n",
      "Requesting offset 400...\n",
      "Retrieved 59 records for 2025-10-31... Total so far: 459\n",
      "Reached dates earlier than 2025-10-31. Stopping.\n",
      "Found 459 records for date 2025-10-31\n",
      "\n",
      "============================================================\n",
      "Searching for BOAMP records with publication date: 2025-10-30\n",
      "============================================================\n",
      "Requesting offset 0...\n",
      "Requesting offset 100...\n",
      "Requesting offset 200...\n",
      "Requesting offset 300...\n",
      "Requesting offset 400...\n",
      "Retrieved 41 records for 2025-10-30... Total so far: 41\n",
      "Requesting offset 500...\n",
      "Retrieved 100 records for 2025-10-30... Total so far: 141\n",
      "Requesting offset 600...\n",
      "Retrieved 100 records for 2025-10-30... Total so far: 241\n",
      "Requesting offset 700...\n",
      "Retrieved 100 records for 2025-10-30... Total so far: 341\n",
      "Requesting offset 800...\n",
      "Retrieved 100 records for 2025-10-30... Total so far: 441\n",
      "Requesting offset 900...\n",
      "Retrieved 92 records for 2025-10-30... Total so far: 533\n",
      "Reached dates earlier than 2025-10-30. Stopping.\n",
      "Found 533 records for date 2025-10-30\n",
      "\n",
      "============================================================\n",
      "Searching for BOAMP records with publication date: 2025-10-29\n",
      "============================================================\n",
      "Requesting offset 0...\n",
      "Requesting offset 100...\n",
      "Requesting offset 200...\n",
      "Requesting offset 300...\n",
      "Requesting offset 400...\n",
      "Requesting offset 500...\n",
      "Requesting offset 600...\n",
      "Requesting offset 700...\n",
      "Requesting offset 800...\n",
      "Requesting offset 900...\n",
      "Retrieved 8 records for 2025-10-29... Total so far: 8\n",
      "Requesting offset 1000...\n",
      "Retrieved 100 records for 2025-10-29... Total so far: 108\n",
      "Requesting offset 1100...\n",
      "Retrieved 100 records for 2025-10-29... Total so far: 208\n",
      "Requesting offset 1200...\n",
      "Retrieved 100 records for 2025-10-29... Total so far: 308\n",
      "Requesting offset 1300...\n",
      "Retrieved 100 records for 2025-10-29... Total so far: 408\n",
      "Requesting offset 1400...\n",
      "Retrieved 55 records for 2025-10-29... Total so far: 463\n",
      "Reached dates earlier than 2025-10-29. Stopping.\n",
      "Found 463 records for date 2025-10-29\n",
      "\n",
      "============================================================\n",
      "Searching for BOAMP records with publication date: 2025-10-28\n",
      "============================================================\n",
      "Requesting offset 0...\n",
      "Requesting offset 100...\n",
      "Requesting offset 200...\n",
      "Requesting offset 300...\n",
      "Requesting offset 400...\n",
      "Requesting offset 500...\n",
      "Requesting offset 600...\n",
      "Requesting offset 700...\n",
      "Requesting offset 800...\n",
      "Requesting offset 900...\n",
      "Requesting offset 1000...\n",
      "Requesting offset 1100...\n",
      "Requesting offset 1200...\n",
      "Requesting offset 1300...\n",
      "Requesting offset 1400...\n",
      "Retrieved 45 records for 2025-10-28... Total so far: 45\n",
      "Requesting offset 1500...\n",
      "Retrieved 100 records for 2025-10-28... Total so far: 145\n",
      "Requesting offset 1600...\n",
      "Retrieved 100 records for 2025-10-28... Total so far: 245\n",
      "Requesting offset 1700...\n",
      "Retrieved 100 records for 2025-10-28... Total so far: 345\n",
      "Requesting offset 1800...\n",
      "Retrieved 8 records for 2025-10-28... Total so far: 353\n",
      "Reached dates earlier than 2025-10-28. Stopping.\n",
      "Found 353 records for date 2025-10-28\n",
      "\n",
      "============================================================\n",
      "Searching for BOAMP records with publication date: 2025-10-27\n",
      "============================================================\n",
      "Requesting offset 0...\n",
      "Requesting offset 100...\n",
      "Requesting offset 200...\n",
      "Requesting offset 300...\n",
      "Requesting offset 400...\n",
      "Requesting offset 500...\n",
      "Requesting offset 600...\n",
      "Requesting offset 700...\n",
      "Requesting offset 800...\n",
      "Requesting offset 900...\n",
      "Requesting offset 1000...\n",
      "Requesting offset 1100...\n",
      "Requesting offset 1200...\n",
      "Requesting offset 1300...\n",
      "Requesting offset 1400...\n",
      "Requesting offset 1500...\n",
      "Requesting offset 1600...\n",
      "Requesting offset 1700...\n",
      "Requesting offset 1800...\n",
      "Retrieved 92 records for 2025-10-27... Total so far: 92\n",
      "Requesting offset 1900...\n",
      "Retrieved 100 records for 2025-10-27... Total so far: 192\n",
      "Requesting offset 2000...\n",
      "Retrieved 17 records for 2025-10-27... Total so far: 209\n",
      "Reached dates earlier than 2025-10-27. Stopping.\n",
      "Found 209 records for date 2025-10-27\n",
      "\n",
      "============================================================\n",
      "‚úÖ EXTRACTION COMPLETE ‚Äî Total 2017 records from 5 dates.\n",
      "üíæ Data saved to boamp_records_20251101_004123.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def get_all_records_for_date(target_date, max_records=5000):\n",
    "    \"\"\"Get all records for a specific date with all available fields\"\"\"\n",
    "    url = \"https://boamp-datadila.opendatasoft.com/api/explore/v2.1/catalog/datasets/boamp/records\"\n",
    "    all_records = []\n",
    "    offset = 0\n",
    "    limit = 100\n",
    "\n",
    "    while len(all_records) < max_records:\n",
    "        params = {\n",
    "            'order_by': 'dateparution DESC',\n",
    "            'limit': limit,\n",
    "            'offset': offset\n",
    "        }\n",
    "\n",
    "        print(f\"Requesting offset {offset}...\")\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        records = data.get('results', [])\n",
    "\n",
    "        if not records:\n",
    "            break  # No more records\n",
    "\n",
    "        # Filter records for our target date\n",
    "        target_records = [record for record in records if record.get('dateparution') == target_date]\n",
    "\n",
    "        # If we found target records, add them\n",
    "        if target_records:\n",
    "            all_records.extend(target_records)\n",
    "            print(f\"Retrieved {len(target_records)} records for {target_date}... Total so far: {len(all_records)}\")\n",
    "\n",
    "        # Check if we've moved past our target date (since we're sorting DESC)\n",
    "        if records and records[-1].get('dateparution', '') < target_date:\n",
    "            print(f\"Reached dates earlier than {target_date}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        offset += limit\n",
    "\n",
    "        if offset > 10000:\n",
    "            print(\"Safety limit reached. Stopping.\")\n",
    "            break\n",
    "\n",
    "    return all_records\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Multiple dates\n",
    "    target_dates = ['2025-10-31', '2025-10-30', '2025-10-29', '2025-10-28', '2025-10-27']\n",
    "    all_results = []\n",
    "\n",
    "    for target_date in target_dates:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Searching for BOAMP records with publication date: {target_date}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        records = get_all_records_for_date(target_date)\n",
    "        print(f\"Found {len(records)} records for date {target_date}\")\n",
    "        all_results.extend(records)\n",
    "\n",
    "    # Save all data in one JSON file\n",
    "    output_file = f\"boamp_records_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ EXTRACTION COMPLETE ‚Äî Total {len(all_results)} records from {len(target_dates)} dates.\")\n",
    "    print(f\"üíæ Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d52b16ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Fetching BOAMP records for: 2025-10-31\n",
      "Requesting offset 0...\n",
      "Retrieved 100 records for 2025-10-31... Total so far: 100\n",
      "Requesting offset 100...\n",
      "Retrieved 100 records for 2025-10-31... Total so far: 200\n",
      "Requesting offset 200...\n",
      "Retrieved 100 records for 2025-10-31... Total so far: 300\n",
      "Requesting offset 300...\n",
      "Retrieved 100 records for 2025-10-31... Total so far: 400\n",
      "Requesting offset 400...\n",
      "Retrieved 59 records for 2025-10-31... Total so far: 459\n",
      "Reached dates earlier than 2025-10-31. Stopping.\n",
      "‚Üí Found 459 records for 2025-10-31\n",
      "\n",
      "============================================================\n",
      "Fetching BOAMP records for: 2025-10-30\n",
      "Requesting offset 0...\n",
      "Requesting offset 100...\n",
      "Requesting offset 200...\n",
      "Requesting offset 300...\n",
      "Requesting offset 400...\n",
      "Retrieved 41 records for 2025-10-30... Total so far: 41\n",
      "Requesting offset 500...\n",
      "Retrieved 100 records for 2025-10-30... Total so far: 141\n",
      "Requesting offset 600...\n",
      "Retrieved 100 records for 2025-10-30... Total so far: 241\n",
      "Requesting offset 700...\n",
      "Retrieved 100 records for 2025-10-30... Total so far: 341\n",
      "Requesting offset 800...\n",
      "Retrieved 100 records for 2025-10-30... Total so far: 441\n",
      "Requesting offset 900...\n",
      "Retrieved 92 records for 2025-10-30... Total so far: 533\n",
      "Reached dates earlier than 2025-10-30. Stopping.\n",
      "‚Üí Found 533 records for 2025-10-30\n",
      "\n",
      "============================================================\n",
      "Fetching BOAMP records for: 2025-10-29\n",
      "Requesting offset 0...\n",
      "Requesting offset 100...\n",
      "Requesting offset 200...\n",
      "Requesting offset 300...\n",
      "Requesting offset 400...\n",
      "Requesting offset 500...\n",
      "Requesting offset 600...\n",
      "Requesting offset 700...\n",
      "Requesting offset 800...\n",
      "Requesting offset 900...\n",
      "Retrieved 8 records for 2025-10-29... Total so far: 8\n",
      "Requesting offset 1000...\n",
      "Retrieved 100 records for 2025-10-29... Total so far: 108\n",
      "Requesting offset 1100...\n",
      "Retrieved 100 records for 2025-10-29... Total so far: 208\n",
      "Requesting offset 1200...\n",
      "Retrieved 100 records for 2025-10-29... Total so far: 308\n",
      "Requesting offset 1300...\n",
      "Retrieved 100 records for 2025-10-29... Total so far: 408\n",
      "Requesting offset 1400...\n",
      "Retrieved 55 records for 2025-10-29... Total so far: 463\n",
      "Reached dates earlier than 2025-10-29. Stopping.\n",
      "‚Üí Found 463 records for 2025-10-29\n",
      "\n",
      "============================================================\n",
      "Fetching BOAMP records for: 2025-10-28\n",
      "Requesting offset 0...\n",
      "Requesting offset 100...\n",
      "Requesting offset 200...\n",
      "Requesting offset 300...\n",
      "Requesting offset 400...\n",
      "Requesting offset 500...\n",
      "Requesting offset 600...\n",
      "Requesting offset 700...\n",
      "Requesting offset 800...\n",
      "Requesting offset 900...\n",
      "Requesting offset 1000...\n",
      "Requesting offset 1100...\n",
      "Requesting offset 1200...\n",
      "Requesting offset 1300...\n",
      "Requesting offset 1400...\n",
      "Retrieved 45 records for 2025-10-28... Total so far: 45\n",
      "Requesting offset 1500...\n",
      "Retrieved 100 records for 2025-10-28... Total so far: 145\n",
      "Requesting offset 1600...\n",
      "Retrieved 100 records for 2025-10-28... Total so far: 245\n",
      "Requesting offset 1700...\n",
      "Retrieved 100 records for 2025-10-28... Total so far: 345\n",
      "Requesting offset 1800...\n",
      "Retrieved 8 records for 2025-10-28... Total so far: 353\n",
      "Reached dates earlier than 2025-10-28. Stopping.\n",
      "‚Üí Found 353 records for 2025-10-28\n",
      "\n",
      "============================================================\n",
      "Fetching BOAMP records for: 2025-10-27\n",
      "Requesting offset 0...\n",
      "Requesting offset 100...\n",
      "Requesting offset 200...\n",
      "Requesting offset 300...\n",
      "Requesting offset 400...\n",
      "Requesting offset 500...\n",
      "Requesting offset 600...\n",
      "Requesting offset 700...\n",
      "Requesting offset 800...\n",
      "Requesting offset 900...\n",
      "Requesting offset 1000...\n",
      "Requesting offset 1100...\n",
      "Requesting offset 1200...\n",
      "Requesting offset 1300...\n",
      "Requesting offset 1400...\n",
      "Requesting offset 1500...\n",
      "Requesting offset 1600...\n",
      "Requesting offset 1700...\n",
      "Requesting offset 1800...\n",
      "Retrieved 92 records for 2025-10-27... Total so far: 92\n",
      "Requesting offset 1900...\n",
      "Retrieved 100 records for 2025-10-27... Total so far: 192\n",
      "Requesting offset 2000...\n",
      "Retrieved 17 records for 2025-10-27... Total so far: 209\n",
      "Reached dates earlier than 2025-10-27. Stopping.\n",
      "‚Üí Found 209 records for 2025-10-27\n",
      "\n",
      "Total records retrieved: 2017\n",
      "üíæ Data saved to Excel file: boamp_records_20251101_004355.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def get_all_records_for_date(target_date, max_records=5000):\n",
    "    \"\"\"Get all records for a specific date with all available fields\"\"\"\n",
    "    url = \"https://boamp-datadila.opendatasoft.com/api/explore/v2.1/catalog/datasets/boamp/records\"\n",
    "    all_records = []\n",
    "    offset = 0\n",
    "    limit = 100\n",
    "\n",
    "    while len(all_records) < max_records:\n",
    "        params = {\n",
    "            'order_by': 'dateparution DESC',\n",
    "            'limit': limit,\n",
    "            'offset': offset\n",
    "        }\n",
    "\n",
    "        print(f\"Requesting offset {offset}...\")\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        records = data.get('results', [])\n",
    "\n",
    "        if not records:\n",
    "            break  # No more records\n",
    "\n",
    "        # Filter records for our target date\n",
    "        target_records = [record for record in records if record.get('dateparution') == target_date]\n",
    "\n",
    "        if target_records:\n",
    "            all_records.extend(target_records)\n",
    "            print(f\"Retrieved {len(target_records)} records for {target_date}... Total so far: {len(all_records)}\")\n",
    "\n",
    "        # Stop if we‚Äôve reached dates earlier than target_date\n",
    "        if records and records[-1].get('dateparution', '') < target_date:\n",
    "            print(f\"Reached dates earlier than {target_date}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        offset += limit\n",
    "\n",
    "        if offset > 10000:\n",
    "            print(\"Safety limit reached. Stopping.\")\n",
    "            break\n",
    "\n",
    "    return all_records\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    target_dates = ['2025-10-31', '2025-10-30', '2025-10-29', '2025-10-28', '2025-10-27']\n",
    "    all_results = []\n",
    "\n",
    "    for target_date in target_dates:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Fetching BOAMP records for: {target_date}\")\n",
    "        records = get_all_records_for_date(target_date)\n",
    "        print(f\"‚Üí Found {len(records)} records for {target_date}\")\n",
    "        all_results.extend(records)\n",
    "\n",
    "    print(f\"\\nTotal records retrieved: {len(all_results)}\")\n",
    "\n",
    "    # ‚úÖ Save results to Excel\n",
    "    if all_results:\n",
    "        df = pd.DataFrame(all_results)\n",
    "        output_file = f\"boamp_records_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "        df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "        print(f\"üíæ Data saved to Excel file: {output_file}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No records found for the given dates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e126d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def get_all_records_for_date(target_date, max_records=5000):\n",
    "    \"\"\"Get all records for a specific date with all available fields\"\"\"\n",
    "    url = \"https://boamp-datadila.opendatasoft.com/api/explore/v2.1/catalog/datasets/boamp/records\"\n",
    "    all_records = []\n",
    "    offset = 0\n",
    "    limit = 100\n",
    "\n",
    "    while len(all_records) < max_records:\n",
    "        params = {\n",
    "            'order_by': 'dateparution DESC',\n",
    "            'limit': limit,\n",
    "            'offset': offset\n",
    "        }\n",
    "\n",
    "        print(f\"Requesting offset {offset}...\")\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        records = data.get('results', [])\n",
    "\n",
    "        if not records:\n",
    "            break  # No more records\n",
    "\n",
    "        # Filter records for our target date\n",
    "        target_records = [record for record in records if record.get('dateparution') == target_date]\n",
    "\n",
    "        if target_records:\n",
    "            all_records.extend(target_records)\n",
    "            print(f\"Retrieved {len(target_records)} records for {target_date}... Total so far: {len(all_records)}\")\n",
    "\n",
    "        # Stop if we‚Äôve reached dates earlier than target_date\n",
    "        if records and records[-1].get('dateparution', '') < target_date:\n",
    "            print(f\"Reached dates earlier than {target_date}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        offset += limit\n",
    "\n",
    "        if offset > 10000:\n",
    "            print(\"Safety limit reached. Stopping.\")\n",
    "            break\n",
    "\n",
    "    return all_records\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    target_dates = ['2025-10-19', '2025-10-18', '2025-10-17', '2025-10-16', '2025-10-15', '2025-10-14', '2025-10-13', '2025-10-12', '2025-10-11', '2025-10-10']\n",
    "    all_results = []\n",
    "\n",
    "    for target_date in target_dates:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Fetching BOAMP records for: {target_date}\")\n",
    "        records = get_all_records_for_date(target_date)\n",
    "        print(f\"‚Üí Found {len(records)} records for {target_date}\")\n",
    "        all_results.extend(records)\n",
    "\n",
    "    print(f\"\\nTotal records retrieved: {len(all_results)}\")\n",
    "\n",
    "    # ‚úÖ Save results to Excel\n",
    "    if all_results:\n",
    "        df = pd.DataFrame(all_results)\n",
    "        output_file = f\"boamp_records_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "        df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "        print(f\"üíæ Data saved to Excel file: {output_file}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No records found for the given dates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25c829f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2025-10-19', '2025-10-18', '2025-10-17', '2025-10-16', '2025-10-15', '2025-10-14', '2025-10-13', '2025-10-12', '2025-10-11', '2025-10-10']\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "dates_octobre = [\n",
    "    (datetime(2025, 10, 19) - timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "    for i in range(10)\n",
    "]\n",
    "\n",
    "print(dates_octobre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a42e20f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Files merged successfully and saved as: D:\\sorabo\\sorabo\\data\\31-10_merged.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths of the two Excel files\n",
    "file1 = r\"D:\\sorabo\\sorabo\\data\\19-10.xlsx\"\n",
    "file2 = r\"D:\\sorabo\\sorabo\\data\\31-20.xlsx\"\n",
    "\n",
    "# Read both Excel files\n",
    "df1 = pd.read_excel(file1)\n",
    "df2 = pd.read_excel(file2)\n",
    "\n",
    "# Combine the two dataframes\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the merged file (overwrite or create new one)\n",
    "output_path = r\"D:\\sorabo\\sorabo\\data\\31-10_merged.xlsx\"\n",
    "merged_df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"‚úÖ Files merged successfully and saved as: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4e1e34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìò D:\\sorabo\\sorabo\\BOAMP_2025-10-31_ALL_KEYWORDS2.xlsx ‚Üí 1119 rows\n",
      "üìò D:\\sorabo\\sorabo\\data\\octobre.xlsx ‚Üí 10000 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "files = [\n",
    "    r\"D:\\sorabo\\sorabo\\BOAMP_2025-10-31_ALL_KEYWORDS2.xlsx\",\n",
    "    r\"D:\\sorabo\\sorabo\\data\\octobre.xlsx\"\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        df = pd.read_excel(file)\n",
    "        print(f\"üìò {file} ‚Üí {len(df)} rows\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File not found: {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c86fb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# CPV d√©taill√©s avec description\n",
      "cpv_list = [\n",
      "    \"44316500-3\", \"Serrurerie\",\n",
      "    \"98395000-8\", \"Services de serrurerie\",\n",
      "    \"44220000-8\", \"Menuiserie pour la construction\",\n",
      "    \"45420000-7\", \"Travaux de menuiserie et de charpenterie\",\n",
      "    \"45421000-4\", \"Travaux de menuiserie\",\n",
      "    \"45421140-7\", \"Pose de menuiseries m√©talliques, except√© portes et fen√™tres\",\n",
      "    \"45421150-0\", \"Travaux d'installation de menuiseries non m√©talliques\",\n",
      "    \"34928200-0\", \"Cl√¥tures\",\n",
      "    \"34928310-4\", \"Cl√¥tures de protection\",\n",
      "    \"45340000-2\", \"Travaux d'installation de cl√¥tures, de garde-corps et de dispositifs de s√©curit√©\",\n",
      "    \"45342000-6\", \"Pose de cl√¥tures\",\n",
      "    \"42416000-5\", \"Ascenseurs, skips, monte-charges, escaliers m√©caniques et trottoirs roulants\",\n",
      "    \"42416400-9\", \"Escaliers m√©caniques\",\n",
      "    \"42419500-1\", \"Pi√®ces pour ascenseurs, skips ou escaliers m√©caniques\",\n",
      "    \"42419530-0\", \"Pi√®ces pour escaliers m√©caniques\",\n",
      "    \"44233000-2\", \"Escaliers\",\n",
      "    \"44423220-9\", \"Escaliers pliants\",\n",
      "    \"45313000-4\", \"Travaux d'installation d'ascenseurs et d'escaliers m√©caniques\",\n",
      "    \"45313200-6\", \"Travaux d'installation d'escaliers m√©caniques\",\n",
      "    \"50740000-4\", \"Services de r√©paration et d'entretien d'escaliers m√©caniques\",\n",
      "    \"51511000-7\", \"Services d'installation de mat√©riel de levage et de manutention, except√© ascenseurs et escaliers m√©caniques\",\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your Excel file\n",
    "file_path = r\"D:\\sorabo\\sorabo\\codesc.xlsx\"\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Use the correct column names\n",
    "code_col = \"CODE\"      # CPV code column\n",
    "desc_col = \"FR\"        # French description column\n",
    "\n",
    "# Create the list\n",
    "cpv_list = []\n",
    "for _, row in df.iterrows():\n",
    "    code = str(row[code_col]).strip()\n",
    "    desc = str(row[desc_col]).strip()\n",
    "    cpv_list.append(code)\n",
    "    cpv_list.append(desc)\n",
    "\n",
    "# Print result in your desired format\n",
    "print(\"\\n# CPV d√©taill√©s avec description\")\n",
    "print(\"cpv_list = [\")\n",
    "for i in range(0, len(cpv_list), 2):\n",
    "    print(f'    \"{cpv_list[i]}\", \"{cpv_list[i+1]}\",')\n",
    "print(\"]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d54db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e56a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Reading Excel file...\n",
      "‚úÖ Found 14 rows for 'miroiterie'\n",
      "‚úÖ Found 110 rows for 'm√©tallerie'\n",
      "‚úÖ Found 33 rows for 'menuiserie ext√©rieure'\n",
      "‚úÖ Found 29 rows for '45420000'\n",
      "‚ö†Ô∏è No matches found for 'Travaux de menuiserie et de charpenterie'.\n",
      "‚úÖ Found 16 rows for '45421100'\n",
      "‚ö†Ô∏è No matches found for 'Pose de portes et de fen√™tres et d'√©l√©ments accessoires'.\n",
      "‚úÖ Found 1 rows for '45421110'\n",
      "‚ö†Ô∏è No matches found for 'Pose d'encadrements de portes et de fen√™tres'.\n",
      "‚úÖ Found 1 rows for '45421111'\n",
      "‚ö†Ô∏è No matches found for 'Pose d'encadrements de portes'.\n",
      "‚úÖ Found 2 rows for '45421112'\n",
      "‚ö†Ô∏è No matches found for 'Pose d'encadrements de fen√™tres'.\n",
      "‚ö†Ô∏è No matches found for '45421120'.\n",
      "‚ö†Ô∏è No matches found for 'Pose de seuils'.\n",
      "‚úÖ Found 15 rows for '45421130'\n",
      "‚ö†Ô∏è No matches found for 'Poses de portes et de fen√™tres'.\n",
      "‚úÖ Found 3 rows for '45421131'\n",
      "‚úÖ Found 1 rows for 'Pose de portes'\n",
      "‚úÖ Found 1 rows for '45421132'\n",
      "‚ö†Ô∏è No matches found for 'Pose de fen√™tres'.\n",
      "‚úÖ Found 17 rows for '45421140'\n",
      "‚ö†Ô∏è No matches found for 'Pose de menuiseries m√©talliques, except√© portes et fen√™tres'.\n",
      "‚úÖ Found 47 rows for '45421141'\n",
      "‚úÖ Found 4 rows for 'Travaux de cloisonnement'\n",
      "‚úÖ Found 1 rows for '45421142'\n",
      "‚ö†Ô∏è No matches found for 'Installation de volets'.\n",
      "‚úÖ Found 10 rows for '45421143'\n",
      "‚ö†Ô∏è No matches found for 'Travaux d'installation de stores'.\n",
      "‚ö†Ô∏è No matches found for '45421144'.\n",
      "‚ö†Ô∏è No matches found for 'Travaux d'installation de v√©lums'.\n",
      "‚úÖ Found 3 rows for '45421145'\n",
      "‚ö†Ô∏è No matches found for 'Travaux d'installation de volets roulants'.\n",
      "‚úÖ Found 90 rows for '44316500'\n",
      "‚úÖ Found 300 rows for 'Serrurerie'\n",
      "‚úÖ Found 3 rows for '98395000'\n",
      "‚ö†Ô∏è No matches found for 'Services de serrurerie'.\n",
      "‚úÖ Found 14 rows for '44220000'\n",
      "‚ö†Ô∏è No matches found for 'Menuiserie pour la construction'.\n",
      "‚úÖ Found 284 rows for '45421000'\n",
      "‚úÖ Found 27 rows for 'Travaux de menuiserie'\n",
      "‚úÖ Found 30 rows for '45421150'\n",
      "‚ö†Ô∏è No matches found for 'Travaux d'installation de menuiseries non m√©talliques'.\n",
      "‚úÖ Found 9 rows for '34928200'\n",
      "‚úÖ Found 62 rows for 'Cl√¥tures'\n",
      "‚úÖ Found 1 rows for '34928310'\n",
      "‚ö†Ô∏è No matches found for 'Cl√¥tures de protection'.\n",
      "‚úÖ Found 18 rows for '45340000'\n",
      "‚ö†Ô∏è No matches found for 'Travaux d'installation de cl√¥tures, de garde-corps et de dispositifs de s√©curit√©'.\n",
      "‚úÖ Found 17 rows for '45342000'\n",
      "‚úÖ Found 12 rows for 'Pose de cl√¥tures'\n",
      "‚úÖ Found 3 rows for '42416000'\n",
      "‚ö†Ô∏è No matches found for 'Ascenseurs, skips, monte-charges, escaliers m√©caniques et trottoirs roulants'.\n",
      "‚ö†Ô∏è No matches found for '42416400'.\n",
      "‚úÖ Found 3 rows for 'Escaliers m√©caniques'\n",
      "‚úÖ Found 1 rows for '42419500'\n",
      "‚ö†Ô∏è No matches found for 'Pi√®ces pour ascenseurs, skips ou escaliers m√©caniques'.\n",
      "‚ö†Ô∏è No matches found for '42419530'.\n",
      "‚ö†Ô∏è No matches found for 'Pi√®ces pour escaliers m√©caniques'.\n",
      "‚ö†Ô∏è No matches found for '44233000'.\n",
      "‚úÖ Found 26 rows for 'Escaliers'\n",
      "‚ö†Ô∏è No matches found for '44423220'.\n",
      "‚ö†Ô∏è No matches found for 'Escaliers pliants'.\n",
      "‚úÖ Found 5 rows for '45313000'\n",
      "‚ö†Ô∏è No matches found for 'Travaux d'installation d'ascenseurs et d'escaliers m√©caniques'.\n",
      "‚úÖ Found 1 rows for '45313200'\n",
      "‚ö†Ô∏è No matches found for 'Travaux d'installation d'escaliers m√©caniques'.\n",
      "‚úÖ Found 2 rows for '50740000'\n",
      "‚ö†Ô∏è No matches found for 'Services de r√©paration et d'entretien d'escaliers m√©caniques'.\n",
      "‚ö†Ô∏è No matches found for '51511000'.\n",
      "‚ö†Ô∏è No matches found for 'Services d'installation de mat√©riel de levage et de manutention, except√© ascenseurs et escaliers m√©caniques'.\n",
      "\n",
      "üìä All matching data saved successfully to:\n",
      "D:\\sorabo\\sorabo\\BOAMP_2025-10-31_ALL_KEYWORDSwithout verefication.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Define your file path ---\n",
    "file_path = r\"D:\\sorabo\\sorabo\\data\\31-10_merged.xlsx\"\n",
    "output_path = r\"D:\\sorabo\\sorabo\\BOAMP_2025-10-31_ALL_KEYWORDS.xlsx\"\n",
    "\n",
    "# --- 2. Define all your keywords and CPV codes ---\n",
    "keywords = [\n",
    "    # Secteurs d‚Äôactivit√©\n",
    "    \"miroiterie\",\n",
    "    \"m√©tallerie\",\n",
    "    \"menuiserie ext√©rieure\",\n",
    "\n",
    "    # CPV simples\n",
    "    \n",
    "   \n",
    "    \n",
    "  \n",
    "\n",
    "    # CPV d√©taill√©s avec description\n",
    "    \"45420000\", \"Travaux de menuiserie et de charpenterie\",\n",
    "    \"45421100\", \"Pose de portes et de fen√™tres et d'√©l√©ments accessoires\",\n",
    "    \"45421110\", \"Pose d'encadrements de portes et de fen√™tres\",\n",
    "    \"45421111\", \"Pose d'encadrements de portes\",\n",
    "    \"45421112\", \"Pose d'encadrements de fen√™tres\",\n",
    "    \"45421120\", \"Pose de seuils\",\n",
    "    \"45421130\", \"Poses de portes et de fen√™tres\",\n",
    "    \"45421131\", \"Pose de portes\",\n",
    "    \"45421132\", \"Pose de fen√™tres\",\n",
    "    \"45421140\", \"Pose de menuiseries m√©talliques, except√© portes et fen√™tres\",\n",
    "    \"45421141\", \"Travaux de cloisonnement\",\n",
    "    \"45421142\", \"Installation de volets\",\n",
    "    \"45421143\", \"Travaux d'installation de stores\",\n",
    "    \"45421144\", \"Travaux d'installation de v√©lums\",\n",
    "    \"45421145\", \"Travaux d'installation de volets roulants\",\n",
    "    \"44316500\", \"Serrurerie\",\n",
    "    \"98395000\", \"Services de serrurerie\",\n",
    "    \"44220000\", \"Menuiserie pour la construction\",\n",
    "    \"45421000\", \"Travaux de menuiserie\",\n",
    "    \"45421140\", \"Pose de menuiseries m√©talliques, except√© portes et fen√™tres\",\n",
    "    \"45421150\", \"Travaux d'installation de menuiseries non m√©talliques\",\n",
    "    \"34928200\", \"Cl√¥tures\",\n",
    "    \"34928310\", \"Cl√¥tures de protection\",\n",
    "    \"45340000\", \"Travaux d'installation de cl√¥tures, de garde-corps et de dispositifs de s√©curit√©\",\n",
    "    \"45342000\", \"Pose de cl√¥tures\",\n",
    "    \"42416000\", \"Ascenseurs, skips, monte-charges, escaliers m√©caniques et trottoirs roulants\",\n",
    "    \"42416400\", \"Escaliers m√©caniques\",\n",
    "    \"42419500\", \"Pi√®ces pour ascenseurs, skips ou escaliers m√©caniques\",\n",
    "    \"42419530\", \"Pi√®ces pour escaliers m√©caniques\",\n",
    "    \"44233000\", \"Escaliers\",\n",
    "    \"44423220\", \"Escaliers pliants\",\n",
    "    \"45313000\", \"Travaux d'installation d'ascenseurs et d'escaliers m√©caniques\",\n",
    "    \"45313200\", \"Travaux d'installation d'escaliers m√©caniques\",\n",
    "    \"50740000\", \"Services de r√©paration et d'entretien d'escaliers m√©caniques\",\n",
    "    \"51511000\", \"Services d'installation de mat√©riel de levage et de manutention, except√© ascenseurs et escaliers m√©caniques\",]\n",
    "# --- 3. Loadxcel ---\n",
    "print(\"üìÇ Reang Excel file...\")\n",
    "df = pd.read_excel(file_path)\n",
    "df_str = df.astype(str).apply(lambda x: x.str.lower())  # for case-insensitive search\n",
    "\n",
    "# --- 4. Create one big dataframe for all results ---\n",
    "all_matches = pd.DataFrame()\n",
    "\n",
    "# --- 5. Loop through each keyword ---\n",
    "for keyword in keywords:\n",
    "    mask = df_str.apply(lambda x: x.str.contains(keyword.lower(), na=False))\n",
    "    filtered_df = df[mask.any(axis=1)]\n",
    "    \n",
    "    if not filtered_df.empty:\n",
    "        print(f\"‚úÖ Found {len(filtered_df)} rows for '{keyword}'\")\n",
    "        filtered_df = filtered_df.copy()\n",
    "        filtered_df[\"keyword\"] = keyword\n",
    "        all_matches = pd.concat([all_matches, filtered_df], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No matches found for '{keyword}'.\")\n",
    "\n",
    "# --- 6. Save everything to one file ---\n",
    "if not all_matches.empty:\n",
    "    all_matches.to_excel(output_path, index=False)\n",
    "    print(f\"\\nüìä All matching data saved successfully to:\\n{output_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No matches found for any keyword.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bc4b6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done! Removed 920 duplicated rows.\n",
      "üíæ Clean file saved to: D:\\sorabo\\sorabo\\data\\31-10_merged_no_duplicates.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ‚úÖ 1. Path to your Excel file\n",
    "file_path = r\"D:\\sorabo\\sorabo\\BOAMP_2025-10-31_ALL_KEYWORDSwithout verefication.xlsx\"\n",
    "\n",
    "\n",
    "# === 2Ô∏è‚É£ Read the Excel file ===\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# === 3Ô∏è‚É£ Column name that contains the ID ===\n",
    "# Change 'CODE' to your actual column name if different\n",
    "id_column = 'id'\n",
    "\n",
    "# === 4Ô∏è‚É£ Find duplicated IDs ===\n",
    "duplicated_ids = df[id_column][df[id_column].duplicated(keep=False)]\n",
    "\n",
    "# === 5Ô∏è‚É£ Remove all rows with those duplicated IDs ===\n",
    "df_clean = df[~df[id_column].isin(duplicated_ids)]\n",
    "\n",
    "# === 6Ô∏è‚É£ Save to a new Excel file ===\n",
    "output_path = r\"D:\\sorabo\\sorabo\\data\\31-10_merged_no_duplicates.xlsx\"\n",
    "df_clean.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Done! Removed {len(df) - len(df_clean)} duplicated rows.\")\n",
    "print(f\"üíæ Clean file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bb0a59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done! Kept 628 unique rows out of 1216 total.\n",
      "üíæ Clean file saved to: D:\\sorabo\\sorabo\\data\\new.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 1Ô∏è‚É£ Path to your Excel file ===\n",
    "file_path = r\"D:\\sorabo\\sorabo\\BOAMP_2025-10-31_ALL_KEYWORDSwithout verefication.xlsx\"\n",
    "\n",
    "# === 2Ô∏è‚É£ Read the Excel file ===\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# === 3Ô∏è‚É£ Column name containing the ID ===\n",
    "# Change this if your ID column has a different name\n",
    "id_column = 'id'\n",
    "\n",
    "# === 4Ô∏è‚É£ Remove duplicate rows ‚Äî keep only the first occurrence per ID ===\n",
    "df_clean = df.drop_duplicates(subset=[id_column], keep='first')\n",
    "\n",
    "# === 5Ô∏è‚É£ Save to a new Excel file ===\n",
    "output_path = r\"D:\\sorabo\\sorabo\\data\\new.xlsx\"\n",
    "df_clean.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Done! Kept {len(df_clean)} unique rows out of {len(df)} total.\")\n",
    "print(f\"üíæ Clean file saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
